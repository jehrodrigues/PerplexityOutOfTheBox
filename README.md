# Perplexity Out-of-the-Box: Measuring intrinsic bias in language models through demographic identity data

Abstract

With the rising popularity of large language models, it becomes more crucial than ever to accurately assess every potential indicator of intrinsic social biases and their propagation downstream, leading to societal injustices and po- tential harms. In this work, we explore the use of demographic/identity data to measure how biased a large language models is when assigning token likelihood and perplexity scores to intersectional demographic axes (e.g. gender, race). A bias measurement dataset containing 13 different demographic axes and 450K unique sentences was adopted, along with a pretrained large language model. The goal was to understand to which extent the model uses demographic/identity descriptors (e.g. rich or poor) as different functions that modify the token’s likeli- hood according to certain contexts, indicating explicit biased behavior. The main contributions are the quantification of intrinsic bias using the bias measurement dataset, the computation of token likelihoods and perplexities using the language model, and the comparison of the proportion of bias across different demographic axes. We show that the adopted language model exhibits a high degree of bias for the Religion (≈87%) and Gender and sex (≈85%) axes, and the lowest degree belongs to the Sexual orientation (≈71%) axis. Exploratory data analysis of the perplexity distribution of Age and Socioeconomic class axes was also conducted.
